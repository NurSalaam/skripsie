{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'rapidsai-csp-utils' already exists and is not an empty directory.\n",
      "\n",
      "\n",
      "\u001b[38;5;57m\u001b[1m⚡️ Tip\u001b[0m\tCheck organization access: \u001b[4mhttps://github.com/settings/connections/applications/c7457225b242a94d60c6\u001b[0m\n",
      "\n",
      "Installing RAPIDS remaining 24.4.* libraries\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Requirement already satisfied: cudf-cu12==24.4.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (24.4.1)\n",
      "Requirement already satisfied: cuml-cu12==24.4.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (24.4.0)\n",
      "Requirement already satisfied: cugraph-cu12==24.4.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (24.4.0)\n",
      "Requirement already satisfied: cuspatial-cu12==24.4.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (24.4.0)\n",
      "Requirement already satisfied: cuproj-cu12==24.4.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (24.4.0)\n",
      "Requirement already satisfied: cuxfilter-cu12==24.4.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (24.4.1)\n",
      "Requirement already satisfied: cucim-cu12==24.4.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (24.4.0)\n",
      "Requirement already satisfied: pylibraft-cu12==24.4.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (24.4.0)\n",
      "Requirement already satisfied: raft-dask-cu12==24.4.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (24.4.0)\n",
      "Requirement already satisfied: nx-cugraph-cu12==24.4.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (24.4.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.10.5)\n",
      "Requirement already satisfied: cachetools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cudf-cu12==24.4.*) (5.5.0)\n",
      "Requirement already satisfied: cuda-python<13.0a0,>=12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cudf-cu12==24.4.*) (12.6.0)\n",
      "Requirement already satisfied: cupy-cuda12x>=12.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cudf-cu12==24.4.*) (13.3.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cudf-cu12==24.4.*) (2024.9.0)\n",
      "Requirement already satisfied: numba>=0.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cudf-cu12==24.4.*) (0.60.0)\n",
      "Requirement already satisfied: numpy<2.0a0,>=1.23 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cudf-cu12==24.4.*) (1.26.4)\n",
      "Requirement already satisfied: nvtx>=0.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cudf-cu12==24.4.*) (0.2.10)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cudf-cu12==24.4.*) (24.1)\n",
      "Requirement already satisfied: pandas<2.2.2dev0,>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cudf-cu12==24.4.*) (2.1.4)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cudf-cu12==24.4.*) (4.23.4)\n",
      "Requirement already satisfied: pynvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cudf-cu12==24.4.*) (0.3.0)\n",
      "Requirement already satisfied: pyarrow<15.0.0a0,>=14.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cudf-cu12==24.4.*) (14.0.2)\n",
      "Requirement already satisfied: rich in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cudf-cu12==24.4.*) (13.8.0)\n",
      "Requirement already satisfied: rmm-cu12==24.4.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cudf-cu12==24.4.*) (24.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cudf-cu12==24.4.*) (4.12.2)\n",
      "Requirement already satisfied: dask-cuda==24.4.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cuml-cu12==24.4.*) (24.4.0)\n",
      "Requirement already satisfied: dask-cudf-cu12==24.4.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cuml-cu12==24.4.*) (24.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cuml-cu12==24.4.*) (1.4.2)\n",
      "Requirement already satisfied: rapids-dask-dependency==24.4.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cuml-cu12==24.4.*) (24.4.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cuml-cu12==24.4.*) (1.11.4)\n",
      "Requirement already satisfied: treelite==4.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cuml-cu12==24.4.*) (4.1.2)\n",
      "Requirement already satisfied: pylibcugraph-cu12==24.4.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cugraph-cu12==24.4.*) (24.4.0)\n",
      "Requirement already satisfied: ucx-py-cu12==0.37.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cugraph-cu12==24.4.*) (0.37.0)\n",
      "Requirement already satisfied: geopandas>=0.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cuspatial-cu12==24.4.*) (1.0.1)\n",
      "Requirement already satisfied: bokeh>=3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cuxfilter-cu12==24.4.*) (3.5.2)\n",
      "Requirement already satisfied: datashader>=0.15 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cuxfilter-cu12==24.4.*) (0.16.3)\n",
      "Requirement already satisfied: holoviews>=1.16.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cuxfilter-cu12==24.4.*) (1.19.1)\n",
      "Requirement already satisfied: jupyter-server-proxy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cuxfilter-cu12==24.4.*) (4.4.0)\n",
      "Requirement already satisfied: panel>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cuxfilter-cu12==24.4.*) (1.5.0)\n",
      "Requirement already satisfied: click in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cucim-cu12==24.4.*) (8.1.7)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cucim-cu12==24.4.*) (0.4)\n",
      "Requirement already satisfied: scikit-image<0.23.0a0,>=0.19.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cucim-cu12==24.4.*) (0.22.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nx-cugraph-cu12==24.4.*) (3.3)\n",
      "Requirement already satisfied: pynvml<11.5,>=11.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from dask-cuda==24.4.*->cuml-cu12==24.4.*) (11.4.1)\n",
      "Requirement already satisfied: zict>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from dask-cuda==24.4.*->cuml-cu12==24.4.*) (3.0.0)\n",
      "Requirement already satisfied: dask==2024.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2024.1.1)\n",
      "Requirement already satisfied: distributed==2024.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2024.1.1)\n",
      "Requirement already satisfied: dask-expr==0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (0.4.0)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (3.0.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (0.12.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (8.5.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (3.1.4)\n",
      "Requirement already satisfied: locket>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (1.0.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (1.1.0)\n",
      "Requirement already satisfied: psutil>=5.7.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (6.0.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.0.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (6.4.1)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2.2.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp) (4.0.3)\n",
      "Requirement already satisfied: contourpy>=1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bokeh>=3.1->cuxfilter-cu12==24.4.*) (1.3.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bokeh>=3.1->cuxfilter-cu12==24.4.*) (10.4.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bokeh>=3.1->cuxfilter-cu12==24.4.*) (2024.9.0)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cupy-cuda12x>=12.0.0->cudf-cu12==24.4.*) (0.8.2)\n",
      "Requirement already satisfied: colorcet in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (3.1.0)\n",
      "Requirement already satisfied: multipledispatch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (1.0.0)\n",
      "Requirement already satisfied: param in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (2.1.1)\n",
      "Requirement already satisfied: pyct in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (0.5.0)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (2.32.3)\n",
      "Requirement already satisfied: xarray in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datashader>=0.15->cuxfilter-cu12==24.4.*) (2024.9.0)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from geopandas>=0.11.0->cuspatial-cu12==24.4.*) (0.9.0)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from geopandas>=0.11.0->cuspatial-cu12==24.4.*) (3.6.1)\n",
      "Requirement already satisfied: shapely>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from geopandas>=0.11.0->cuspatial-cu12==24.4.*) (2.0.6)\n",
      "Requirement already satisfied: pyviz-comms>=2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from holoviews>=1.16.0->cuxfilter-cu12==24.4.*) (3.0.3)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from numba>=0.57->cudf-cu12==24.4.*) (0.43.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas<2.2.2dev0,>=2.0->cudf-cu12==24.4.*) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas<2.2.2dev0,>=2.0->cudf-cu12==24.4.*) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas<2.2.2dev0,>=2.0->cudf-cu12==24.4.*) (2024.1)\n",
      "Requirement already satisfied: bleach in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (6.1.0)\n",
      "Requirement already satisfied: linkify-it-py in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (2.0.3)\n",
      "Requirement already satisfied: markdown in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (3.7)\n",
      "Requirement already satisfied: markdown-it-py in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (3.0.0)\n",
      "Requirement already satisfied: mdit-py-plugins in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (0.4.2)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from panel>=1.0->cuxfilter-cu12==24.4.*) (4.66.1)\n",
      "Requirement already satisfied: imageio>=2.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image<0.23.0a0,>=0.19.0->cucim-cu12==24.4.*) (2.35.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image<0.23.0a0,>=0.19.0->cucim-cu12==24.4.*) (2024.8.30)\n",
      "Requirement already satisfied: idna>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp) (3.8)\n",
      "Requirement already satisfied: jupyter-server>=1.24.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2.14.2)\n",
      "Requirement already satisfied: simpervisor>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.0.0)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server-proxy->cuxfilter-cu12==24.4.*) (5.14.3)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->cudf-cu12==24.4.*) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2>=2.10.3->distributed==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (2.1.5)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (4.4.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (8.6.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (7.16.4)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.20.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (26.2.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.8.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py->panel>=1.0->cuxfilter-cu12==24.4.*) (0.1.2)\n",
      "Requirement already satisfied: certifi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyogrio>=0.7.2->geopandas>=0.11.0->cuspatial-cu12==24.4.*) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<2.2.2dev0,>=2.0->cudf-cu12==24.4.*) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bleach->panel>=1.0->cuxfilter-cu12==24.4.*) (0.5.1)\n",
      "Requirement already satisfied: uc-micro-py in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from linkify-it-py->panel>=1.0->cuxfilter-cu12==24.4.*) (1.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->datashader>=0.15->cuxfilter-cu12==24.4.*) (3.3.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.2.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (21.2.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask==2024.1.1->rapids-dask-dependency==24.4.*->cuml-cu12==24.4.*) (3.20.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (4.3.2)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (4.23.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2.0.7)\n",
      "Requirement already satisfied: referencing in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.35.1)\n",
      "Requirement already satisfied: rfc3339-validator in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (4.12.3)\n",
      "Requirement already satisfied: defusedxml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.10.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.3.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbformat>=5.3.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2.20.0)\n",
      "Requirement already satisfied: ptyprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from terminado>=0.8.3->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.7.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2023.12.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (0.20.0)\n",
      "Requirement already satisfied: fqdn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (3.0.0)\n",
      "Requirement already satisfied: uri-template in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (24.8.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2.6)\n",
      "Requirement already satisfied: pycparser in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==24.4.*) (2.9.0.20240906)\n",
      "\n",
      "        ***********************************************************************\n",
      "        The pip install of RAPIDS is complete.\n",
      "        \n",
      "        Please do not run any further installation from the conda based installation methods, as they may cause issues!\n",
      "        \n",
      "        Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts.\n",
      "\n",
      "        Troubleshooting:\n",
      "            - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files. \n",
      "            - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n",
      "        ***********************************************************************\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "!python rapidsai-csp-utils/colab/pip-install.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some GPU Megiks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.04.01\n",
      "24.04.00\n",
      "24.04.00\n",
      "24.04.00\n",
      "24.04.01\n"
     ]
    }
   ],
   "source": [
    "import cudf\n",
    "print(cudf.__version__)\n",
    "\n",
    "import cuml\n",
    "print(cuml.__version__)\n",
    "\n",
    "import cugraph\n",
    "print(cugraph.__version__)\n",
    "\n",
    "import cuspatial\n",
    "print(cuspatial.__version__)\n",
    "\n",
    "import cuxfilter\n",
    "print(cuxfilter.__version__)\n",
    "\n",
    "%load_ext cudf.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "for d in os.listdir('scores'):\n",
    "    x = os.listdir(os.path.join('scores', d))\n",
    "    l = len(x)\n",
    "    n = len(set(x))\n",
    "    if (l!=250) or (n!=250):\n",
    "        print(f'Dir: {d}')\n",
    "        print(f'#files: {l}')\n",
    "        print(f\"#unique: {n}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import applyWindowing, reverseWindowing\n",
    "from dataloader import DataLoader\n",
    "# from lof import LocalOutlierFactor\n",
    "from ocsvm import OCSVM\n",
    "from benchmarker import benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True\n",
    "UCR = 'ucrdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lof import LocalOutlierFactor\n",
    "from isolation_forest import IsolationForest\n",
    "from utilities import applyWindowing, reverseWindowing\n",
    "from dataloader import TimeSeries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "\n",
    "class IFLOFMultiplicativeEnsemble:\n",
    "    '''\n",
    "    ensemble = IFLOFMultiplicativeEnsemble(window_size=100, if_estimators=100, lof_neighbors=20)\n",
    "    ensemble.fit(train_ts)\n",
    "    result_ts, processing_time = ensemble.predict(test_ts)\n",
    "    '''\n",
    "    def __init__(self, window_size=50, if_estimators=100, lof_neighbors=50):\n",
    "        self.window_size = window_size\n",
    "        self.if_estimators = if_estimators\n",
    "        self.lof_neighbors = lof_neighbors\n",
    "        self.iforest = IsolationForest(windowSize=window_size, n_estimators=if_estimators)\n",
    "        self.lof = LocalOutlierFactor(windowSize=window_size, neighbors=lof_neighbors)\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def fit(self, tsObject: TimeSeries):\n",
    "        self.iforest.fit(tsObject)\n",
    "\n",
    "    def predict(self, tsObject: TimeSeries):\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Apply windowing\n",
    "        windowed_data = applyWindowing(tsObject.testData.values, self.window_size)\n",
    "\n",
    "        # Pass windowed data to IF, get anomaly scores\n",
    "        if_result, _ = self.iforest.predict(tsObject)\n",
    "        if_scores = if_result.testData[\"Score\"].values\n",
    "\n",
    "        # Normalize IF scores to [0,1]\n",
    "        if_scores_normalized = self.scaler.fit_transform(if_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Pass windowed data to LOF, get anomaly scores\n",
    "        lof_result, _ = self.lof.predict(tsObject)\n",
    "        lof_scores = lof_result.testData[\"Score\"].values\n",
    "\n",
    "        # Normalize LOF scores to [0,1]\n",
    "        lof_scores_normalized = self.scaler.fit_transform(lof_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Compute final scores as: Final_Score = IF_Score * LOF_Score\n",
    "        final_scores = if_scores_normalized * lof_scores_normalized\n",
    "\n",
    "        processing_time = time.time() - start_time\n",
    "\n",
    "        # Identify the point with maximum combined score\n",
    "        # anomaly_index = np.argmax(final_scores)\n",
    "\n",
    "        # Update tsObject with final scores\n",
    "        tsObject.testData[\"Score\"] = final_scores\n",
    "\n",
    "        return tsObject, processing_time\n",
    "\n",
    "    def toString(self):\n",
    "        return f\"iflof_mult_w{self.window_size}_e{self.if_estimators}_n{self.lof_neighbors}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: False\n"
     ]
    }
   ],
   "source": [
    "filename = os.listdir(UCR)[0]\n",
    "file_path = os.path.join(UCR, filename)\n",
    "\n",
    "dataloader = DataLoader()\n",
    "ts = dataloader.load_file(file_path)\n",
    "\n",
    "ts\n",
    "\n",
    "detector = IFLOFMultiplicativeEnsemble()\n",
    "detector.fit(ts)\n",
    "ts, t = detector.predict(ts)\n",
    "\n",
    "scores = ts.testData[\"Score\"]\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "correct = ts.anomalies[0].start <= prediction <= ts.anomalies[0].end\n",
    "print(f\"Correct: {correct}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking:   0%|          | 1/250 [00:15<1:04:00, 15.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m results_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mtoString()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m scores_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator\u001b[38;5;241m.\u001b[39mtoString())\n\u001b[0;32m----> 5\u001b[0m \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUCR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/benchmarker.py:42\u001b[0m, in \u001b[0;36mbenchmark\u001b[0;34m(algorithm, path_to_benchmark, path_to_save_results_file, path_to_save_scores, save_scores)\u001b[0m\n\u001b[1;32m     39\u001b[0m algorithm\u001b[38;5;241m.\u001b[39mfit(ts)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m ts, processing_time \u001b[38;5;241m=\u001b[39m \u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m total_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m processing_time\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Get prediction\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m, in \u001b[0;36mIFMPEnsemble.predict\u001b[0;34m(self, tsObject)\u001b[0m\n\u001b[1;32m     25\u001b[0m mask \u001b[38;5;241m=\u001b[39m (if_scores \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Step 5-6: Apply MP\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m mp_result, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtsObject\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m mp_scores \u001b[38;5;241m=\u001b[39m mp_result\u001b[38;5;241m.\u001b[39mtestData[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Ensure mp_scores and if_scores have the same length\u001b[39;00m\n",
      "File \u001b[0;32m~/matrix_profile.py:81\u001b[0m, in \u001b[0;36mMatrixProfile.predict\u001b[0;34m(self, tsObject)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     data \u001b[38;5;241m=\u001b[39m tsObject\u001b[38;5;241m.\u001b[39mtestData\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 81\u001b[0m profile, index \u001b[38;5;241m=\u001b[39m \u001b[43mpyscamp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselfjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindowSize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m scores \u001b[38;5;241m=\u001b[39m profile\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Check for NaN values in the profile\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/cudf/pandas/fast_slow_proxy.py:417\u001b[0m, in \u001b[0;36m_FastSlowAttribute.__get__\u001b[0;34m(self, obj, owner)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(obj, _FastSlowProxy)\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mtype\u001b[39m(obj), _FastSlowProxyMeta)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;66;03m# fast/slow objects for instances of _FastSlowProxy or\u001b[39;00m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m# subtypes of _FastSlowProxyMeta:\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     _raise_attribute_error(owner \u001b[38;5;28;01mif\u001b[39;00m owner \u001b[38;5;28;01melse\u001b[39;00m obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name)\n\u001b[0;32m--> 417\u001b[0m result, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_fast_slow_function_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, functools\u001b[38;5;241m.\u001b[39mcached_property):\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;66;03m# TODO: temporary workaround until dask is able\u001b[39;00m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;66;03m# to correctly inspect cached_property objects.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# GH: 264\u001b[39;00m\n\u001b[1;32m    423\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mproperty\u001b[39m(result\u001b[38;5;241m.\u001b[39mfunc)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/cudf/pandas/fast_slow_proxy.py:888\u001b[0m, in \u001b[0;36m_fast_slow_function_call\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m nvtx\u001b[38;5;241m.\u001b[39mannotate(\n\u001b[1;32m    884\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEXECUTE_FAST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    885\u001b[0m         color\u001b[38;5;241m=\u001b[39m_CUDF_PANDAS_NVTX_COLORS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEXECUTE_FAST\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    886\u001b[0m         domain\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcudf_pandas\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    887\u001b[0m     ):\n\u001b[0;32m--> 888\u001b[0m         fast_args, fast_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43m_fast_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m, _fast_arg(kwargs)\n\u001b[1;32m    889\u001b[0m         result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39mfast_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfast_kwargs)\n\u001b[1;32m    890\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    891\u001b[0m             \u001b[38;5;66;03m# try slow path\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimator = IFLOFMultiplicativeEnsemble()\n",
    "results_file = os.path.join('.', \"results\", f\"{estimator.toString()}.csv\")\n",
    "scores_dir = os.path.join('.', \"scores\", estimator.toString())\n",
    "\n",
    "benchmark(estimator, UCR, results_file, scores_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOF detectors\n",
    "\n",
    "First, a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = os.listdir(UCR)[0]\n",
    "# file_path = os.path.join(UCR, filename)\n",
    "\n",
    "# dataloader = DataLoader()\n",
    "# ts = dataloader.load_file(file_path)\n",
    "\n",
    "# ts\n",
    "\n",
    "# detector = OCSVM(windowSize=10)\n",
    "# detector.fit(ts)\n",
    "# ts, t = detector.predict(ts)\n",
    "\n",
    "# scores = ts.testData[\"Scores\"]\n",
    "# prediction = np.argmax(scores)\n",
    "\n",
    "# correct = ts.anomalies[0].start <= prediction <= ts.anomalies[0].end\n",
    "# print(f\"Correct: {correct}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lof = LocalOutlierFactor(windowSize=50, neighbors=50, gpu=True)\n",
    "# results_file = os.path.join('.', \"results\", f\"{lof.toString()}.csv\")\n",
    "# scores_dir = os.path.join('.', \"results\", lof.toString(), \"scores\")\n",
    "\n",
    "# benchmark(lof, UCR, results_file, scores_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WV-OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def wv_knn(scores, k=3):\n",
    "    \"\"\"\n",
    "    Weighted Voting with KNN\n",
    "    \n",
    "    :param scores: numpy array of shape (n_samples, n_detectors)\n",
    "    :param k: number of neighbors for KNN\n",
    "    :return: numpy array of final anomaly scores\n",
    "    \"\"\"\n",
    "    n_samples, n_detectors = scores.shape\n",
    "    weights = np.zeros(n_detectors)\n",
    "    \n",
    "    for d in range(n_detectors):\n",
    "        X = np.delete(scores, d, axis=1)\n",
    "        y = scores[:, d]\n",
    "        \n",
    "        knn = KNeighborsRegressor(n_neighbors=k)\n",
    "        knn.fit(X, y)\n",
    "        y_pred = knn.predict(X)\n",
    "        \n",
    "        weights[d] = r2_score(y, y_pred)\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = weights / np.sum(weights)\n",
    "    \n",
    "    # Compute final anomaly scores\n",
    "    final_scores = np.max(weights * scores, axis=1)\n",
    "    \n",
    "    return final_scores, weights\n",
    "\n",
    "def wv_ols(scores):\n",
    "    \"\"\"\n",
    "    Weighted Voting with Ordinary Least Squares\n",
    "    \n",
    "    :param scores: numpy array of shape (n_samples, n_detectors)\n",
    "    :return: numpy array of final anomaly scores\n",
    "    \"\"\"\n",
    "    n_samples, n_detectors = scores.shape\n",
    "    weights = np.zeros(n_detectors)\n",
    "    \n",
    "    for d in range(n_detectors):\n",
    "        X = np.delete(scores, d, axis=1)\n",
    "        y = scores[:, d]\n",
    "        \n",
    "        ols = LinearRegression()\n",
    "        ols.fit(X, y)\n",
    "        y_pred = ols.predict(X)\n",
    "        \n",
    "        rmse = np.sqrt(np.mean((y - y_pred)**2))\n",
    "        weights[d] = max(0, 1 - rmse)\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = weights / np.sum(weights)\n",
    "    \n",
    "    # Compute final anomaly scores\n",
    "    final_scores = np.max(weights * scores, axis=1)\n",
    "    \n",
    "    return final_scores, weights\n",
    "\n",
    "# def wv_ols_r2(scores):\n",
    "#     \"\"\"\n",
    "#     Weighted Voting with Ordinary Least Squares using R^2\n",
    "    \n",
    "#     :param scores: numpy array of shape (n_samples, n_detectors)\n",
    "#     :return: numpy array of final anomaly scores, and weights\n",
    "#     \"\"\"\n",
    "#     n_samples, n_detectors = scores.shape\n",
    "#     weights = np.zeros(n_detectors)\n",
    "    \n",
    "#     for d in range(n_detectors):\n",
    "#         X = np.delete(scores, d, axis=1)\n",
    "#         y = scores[:, d]\n",
    "        \n",
    "#         ols = LinearRegression()\n",
    "#         ols.fit(X, y)\n",
    "#         y_pred = ols.predict(X)\n",
    "        \n",
    "#         weights[d] = r2_score(y, y_pred)\n",
    "    \n",
    "#     # Normalize weights\n",
    "#     weights = weights / np.sum(weights)\n",
    "    \n",
    "#     # Compute final anomaly scores\n",
    "#     final_scores = np.max(weights * scores, axis=1)\n",
    "    \n",
    "#     return final_scores, weights    \n",
    "\n",
    "def wv_ols_r2(scores, emphasize_diversity=False):\n",
    "    \"\"\"\n",
    "    Weighted Voting with Ordinary Least Squares using R^2\n",
    "    \n",
    "    :param scores: numpy array of shape (n_samples, n_detectors)\n",
    "    :param emphasize_diversity: if True, give higher weights to less predictable detectors\n",
    "    :return: numpy array of final anomaly scores, and weights\n",
    "    \"\"\"\n",
    "    n_samples, n_detectors = scores.shape\n",
    "    weights = np.zeros(n_detectors)\n",
    "    \n",
    "    for d in range(n_detectors):\n",
    "        X = np.delete(scores, d, axis=1)\n",
    "        y = scores[:, d]\n",
    "        \n",
    "        ols = LinearRegression()\n",
    "        ols.fit(X, y)\n",
    "        y_pred = ols.predict(X)\n",
    "        \n",
    "        weights[d] = r2_score(y, y_pred)\n",
    "    \n",
    "    if emphasize_diversity:\n",
    "        # Invert weights to emphasize diversity\n",
    "        weights = 1 - weights\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    weights = np.maximum(weights, 1e-10)\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = weights / np.sum(weights)\n",
    "    \n",
    "    # Compute final anomaly scores\n",
    "    final_scores = np.max(weights * scores, axis=1)\n",
    "    \n",
    "    return final_scores, weights\n",
    "\n",
    "# # Example usage\n",
    "# np.random.seed(42)\n",
    "# n_samples = 1000\n",
    "# n_detectors = 5\n",
    "\n",
    "# # Simulating anomaly scores from multiple detectors\n",
    "# scores = np.random.rand(n_samples, n_detectors)\n",
    "\n",
    "# # Apply the method with original assumption (a)\n",
    "# final_scores_a, weights_a = wv_ols_r2(scores, emphasize_diversity=False)\n",
    "\n",
    "# # Apply the method with assumption (b) to emphasize diversity\n",
    "# final_scores_b, weights_b = wv_ols_r2(scores, emphasize_diversity=True)\n",
    "\n",
    "\n",
    "\n",
    "def wv_ols_r2_topk(scores, k):\n",
    "    \"\"\"\n",
    "    Voting with top-k detectors selected based on R^2\n",
    "    \n",
    "    :param scores: numpy array of shape (n_samples, n_detectors)\n",
    "    :param k: number of top detectors to use\n",
    "    :return: numpy array of final anomaly scores, and selected detector indices\n",
    "    \"\"\"\n",
    "    n_samples, n_detectors = scores.shape\n",
    "    r2_scores = np.zeros(n_detectors)\n",
    "    \n",
    "    for d in range(n_detectors):\n",
    "        X = np.delete(scores, d, axis=1)\n",
    "        y = scores[:, d]\n",
    "        \n",
    "        ols = LinearRegression()\n",
    "        ols.fit(X, y)\n",
    "        y_pred = ols.predict(X)\n",
    "        \n",
    "        r2_scores[d] = r2_score(y, y_pred)\n",
    "    \n",
    "    # Select top-k detectors (lower R^2 means more unique information)\n",
    "    top_k_indices = np.argsort(r2_scores)[:k]\n",
    "    \n",
    "    # Compute final anomaly scores (average of top-k detectors)\n",
    "    final_scores = np.mean(scores[:, top_k_indices], axis=1)\n",
    "    \n",
    "    return final_scores, top_k_indices\n",
    "\n",
    "# # Example usage\n",
    "# np.random.seed(42)\n",
    "# n_samples = 1000\n",
    "# n_detectors = 5\n",
    "\n",
    "# # Simulating anomaly scores from multiple detectors\n",
    "# scores = np.random.rand(n_samples, n_detectors)\n",
    "\n",
    "# # Apply the new method\n",
    "# final_scores, selected_detectors = wv_ols_r2_topk(scores, k=3)\n",
    "\n",
    "# print(\"Selected detectors:\", selected_detectors)\n",
    "\n",
    "# # Visualize results\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.plot(final_scores, label='Top-k R^2 Voting')\n",
    "# plt.legend()\n",
    "# plt.title('Anomaly Scores')\n",
    "# plt.xlabel('Sample')\n",
    "# plt.ylabel('Anomaly Score')\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.bar(range(n_detectors), [1 if i in selected_detectors else 0 for i in range(n_detectors)])\n",
    "# plt.title('Selected Detectors')\n",
    "# plt.xlabel('Detector Index')\n",
    "# plt.ylabel('Selected (1) / Not Selected (0)')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# base_learners = os.listdir('scores')\n",
    "# dataloader = DataLoader()\n",
    "\n",
    "# time_series = []\n",
    "# status = []\n",
    "\n",
    "# for f in tqdm(os.listdir(UCR)):\n",
    "#     ts_name = f.split('.')[0]\n",
    "#     base_learner_scores = []\n",
    "\n",
    "#     for base_learner in base_learners:\n",
    "#         base_learner_scores.append(pd.read_csv(f'scores/{base_learner}/{ts_name}_scores.csv').to_numpy())\n",
    "\n",
    "#     scores = np.column_stack(tuple(base_learner_scores))\n",
    "#     final_scores =  np.mean(scores, axis=1)\n",
    "\n",
    "#     df_final_scores = pd.DataFrame(final_scores, columns=['Score'])\n",
    "#     prediction = np.argmax(final_scores)\n",
    "\n",
    "#     ts = dataloader.load_file(file_path = os.path.join(UCR, f'{ts_name}.csv'))\n",
    "#     if (ts.anomalies[0].start <= prediction <= ts.anomalies[0].end):\n",
    "#         status.append(True)\n",
    "#     else:\n",
    "#         status.append(False)\n",
    "\n",
    "#     time_series.append(f\"{ts_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results = pd.DataFrame({'name':time_series, 'status':status})\n",
    "# accuracy = df_results['status'].sum()/df_results.shape[0]\n",
    "# print(f'Accuracy: {accuracy}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WV-OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# base_learners = os.listdir('scores')\n",
    "# dataloader = DataLoader()\n",
    "\n",
    "# time_series = []\n",
    "# status = []\n",
    "\n",
    "# for f in tqdm(os.listdir(UCR)):\n",
    "#     ts_name = f.split('.')[0]\n",
    "#     base_learner_scores = []\n",
    "\n",
    "#     for base_learner in base_learners:\n",
    "#         base_learner_scores.append(pd.read_csv(f'scores/{base_learner}/{ts_name}_scores.csv').to_numpy())\n",
    "\n",
    "#     scores = np.column_stack(tuple(base_learner_scores))\n",
    "#     final_scores, weights = wv_ols(scores)\n",
    "\n",
    "\n",
    "#     df_final_scores = pd.DataFrame(final_scores, columns=['Score'])\n",
    "#     ts = dataloader.load_file(file_path = os.path.join(UCR, f'{ts_name}.csv'))\n",
    "#     prediction = np.argmax(final_scores)\n",
    "\n",
    "#     if (ts.anomalies[0].start <= prediction <= ts.anomalies[0].end):\n",
    "#         status.append(True)\n",
    "#     else:\n",
    "#         status.append(False)\n",
    "\n",
    "#     time_series.append(f\"{ts_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results = pd.DataFrame({'name':time_series, 'status':status})\n",
    "# accuracy = df_results['status'].sum()/df_results.shape[0]\n",
    "# print(f'Accuracy: {accuracy}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WV_OLS_R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# base_learners = os.listdir('scores')\n",
    "# dataloader = DataLoader()\n",
    "\n",
    "# time_series = []\n",
    "# status = []\n",
    "\n",
    "# for f in tqdm(os.listdir(UCR)):\n",
    "#     ts_name = f.split('.')[0]\n",
    "#     base_learner_scores = []\n",
    "\n",
    "#     for base_learner in base_learners:\n",
    "#         base_learner_scores.append(pd.read_csv(f'scores/{base_learner}/{ts_name}_scores.csv').to_numpy())\n",
    "\n",
    "#     scores = np.column_stack(tuple(base_learner_scores))\n",
    "#     final_scores, weights = wv_ols_r2(scores)\n",
    "\n",
    "\n",
    "#     df_final_scores = pd.DataFrame(final_scores, columns=['Score'])\n",
    "#     ts = dataloader.load_file(file_path = os.path.join(UCR, f'{ts_name}.csv'))\n",
    "#     prediction = np.argmax(final_scores)\n",
    "\n",
    "#     if (ts.anomalies[0].start <= prediction <= ts.anomalies[0].end):\n",
    "#         status.append(True)\n",
    "#     else:\n",
    "#         status.append(False)\n",
    "\n",
    "#     time_series.append(f\"{ts_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results = pd.DataFrame({'name':time_series, 'status':status})\n",
    "# accuracy = df_results['status'].sum()/df_results.shape[0]\n",
    "# print(f'Accuracy: {accuracy}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_learners = os.listdir('scores')\n",
    "# dataloader = DataLoader()\n",
    "\n",
    "# time_series = []\n",
    "# status = []\n",
    "\n",
    "# for f in tqdm(os.listdir(UCR)):\n",
    "#     ts_name = f.split('.')[0]\n",
    "#     base_learner_scores = []\n",
    "\n",
    "#     for base_learner in base_learners:\n",
    "#         base_learner_scores.append(pd.read_csv(f'scores/{base_learner}/{ts_name}_scores.csv').to_numpy())\n",
    "\n",
    "#     scores = np.column_stack(tuple(base_learner_scores))\n",
    "#     final_scores, weights = wv_knn(scores)\n",
    "\n",
    "\n",
    "#     df_final_scores = pd.DataFrame(final_scores, columns=['Score'])\n",
    "#     ts = dataloader.load_file(file_path = os.path.join(UCR, f'{ts_name}.csv'))\n",
    "#     prediction = np.argmax(final_scores)\n",
    "\n",
    "#     if (ts.anomalies[0].start <= prediction <= ts.anomalies[0].end):\n",
    "#         status.append(True)\n",
    "#     else:\n",
    "#         status.append(False)\n",
    "\n",
    "#     time_series.append(f\"{ts_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WV_OLS_R2_DIVERSITY_TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# base_learners = os.listdir('scores')\n",
    "# dataloader = DataLoader()\n",
    "\n",
    "# time_series = []\n",
    "# status = []\n",
    "\n",
    "# for f in tqdm(os.listdir(UCR)):\n",
    "#     ts_name = f.split('.')[0]\n",
    "#     base_learner_scores = []\n",
    "\n",
    "#     for base_learner in base_learners:\n",
    "#         base_learner_scores.append(pd.read_csv(f'scores/{base_learner}/{ts_name}_scores.csv').to_numpy())\n",
    "\n",
    "#     scores = np.column_stack(tuple(base_learner_scores))\n",
    "#     final_scores, weights = wv_ols_r2(scores, emphasize_diversity=True)\n",
    "\n",
    "\n",
    "#     df_final_scores = pd.DataFrame(final_scores, columns=['Score'])\n",
    "#     ts = dataloader.load_file(file_path = os.path.join(UCR, f'{ts_name}.csv'))\n",
    "#     prediction = np.argmax(final_scores)\n",
    "\n",
    "#     if (ts.anomalies[0].start <= prediction <= ts.anomalies[0].end):\n",
    "#         status.append(True)\n",
    "#     else:\n",
    "#         status.append(False)\n",
    "\n",
    "#     time_series.append(f\"{ts_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results = pd.DataFrame({'name':time_series, 'status':status})\n",
    "# accuracy = df_results['status'].sum()/df_results.shape[0]\n",
    "# print(f'Accuracy: {accuracy}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary experiments showed that this was a kak idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# base_learners = os.listdir('scores')\n",
    "# dataloader = DataLoader()\n",
    "\n",
    "# time_series = []\n",
    "# status = []\n",
    "\n",
    "# for f in tqdm(os.listdir(UCR)):\n",
    "#     ts_name = f.split('.')[0]\n",
    "#     base_learner_scores = []\n",
    "\n",
    "#     for base_learner in base_learners:\n",
    "#         base_learner_scores.append(pd.read_csv(f'scores/{base_learner}/{ts_name}_scores.csv').to_numpy())\n",
    "\n",
    "#     scores = np.column_stack(tuple(base_learner_scores))\n",
    "#     final_scores, weights = wv_ols_r2_topk(scores, 10)\n",
    "\n",
    "\n",
    "#     df_final_scores = pd.DataFrame(final_scores, columns=['Score'])\n",
    "#     ts = dataloader.load_file(file_path = os.path.join(UCR, f'{ts_name}.csv'))\n",
    "#     prediction = np.argmax(final_scores)\n",
    "\n",
    "#     if (ts.anomalies[0].start <= prediction <= ts.anomalies[0].end):\n",
    "#         status.append(True)\n",
    "#     else:\n",
    "#         status.append(False)\n",
    "\n",
    "#     time_series.append(f\"{ts_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results = pd.DataFrame({'name':time_series, 'status':status})\n",
    "# accuracy = df_results['status'].sum()/df_results.shape[0]\n",
    "# print(f'Accuracy: {accuracy}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# if_meta_detector = IsolationForest()\n",
    "\n",
    "# base_learners = os.listdir('scores')\n",
    "# dataloader = DataLoader()\n",
    "\n",
    "# time_series = []\n",
    "# status = []\n",
    "\n",
    "# for f in tqdm(os.listdir(UCR)):\n",
    "#     ts_name = f.split('.')[0]\n",
    "#     base_learner_scores = []\n",
    "\n",
    "#     for base_learner in base_learners:\n",
    "#         base_learner_scores.append(pd.read_csv(f'scores/{base_learner}/{ts_name}_scores.csv').to_numpy())\n",
    "\n",
    "#     scores = np.column_stack(tuple(base_learner_scores))\n",
    "#     if_meta_detector.fit(scores)\n",
    "#     # NB! THIS IS WRONG! THE DECISION FUNCTION VALUES NEED TO BE USED AS WEIGHTS FOR THE DETECTORS\n",
    "#     final_scores = -if_meta_detector.decision_function(scores)\n",
    "\n",
    "#     df_final_scores = pd.DataFrame(final_scores, columns=['Score'])\n",
    "#     prediction = np.argmax(final_scores)\n",
    "\n",
    "#     ts = dataloader.load_file(file_path = os.path.join(UCR, f'{ts_name}.csv'))\n",
    "#     if (ts.anomalies[0].start <= prediction <= ts.anomalies[0].end):\n",
    "#         status.append(True)\n",
    "#     else:\n",
    "#         status.append(False)\n",
    "\n",
    "#     time_series.append(f\"{ts_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results = pd.DataFrame({'name':time_series, 'status':status})\n",
    "# accuracy = df_results['status'].sum()/df_results.shape[0]\n",
    "# print(f'Accuracy: {accuracy}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneClassSVM\n",
    "\n",
    "remember this is the shape of the input data to your ONCSVM.\n",
    "\n",
    "```\n",
    "t | d1 | d2 | d3 | ... | dn\n",
    "```\n",
    "\n",
    "It assigns greater _weight_ to the detector with the most **different** score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB! NB! NB!\n",
    "USE THE \"UN-CORRELATEDNESS\" AS A MEASURE OF CAPUTRING INFORMATION, AND THEN ONLY SELECT THE TOP-K MOST UNCORRELATED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB! LOF(50, 50) RESULTS FILE IS INVALID.  SCORES ARE STILL VALID.  NEED TO RE-CACLUCLATE RESULTS FILE ONLY!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expermient 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# from lof import LocalOutlierFactor\n",
    "# from matrix_profile import MatrixProfile\n",
    "# from isolation_forest import IsolationForest\n",
    "# from kmeans import KMeans\n",
    "# # Import other necessary modules (MP, etc.)\n",
    "\n",
    "# from dataloader import DataLoader\n",
    "# from benchmarker import benchmark, process_precomputed_scores\n",
    "# import os\n",
    "# import itertools\n",
    "\n",
    "# # Define base learners and their parameter ranges\n",
    "# base_learners = {\n",
    "#     'LOF': {\n",
    "#         'class': LocalOutlierFactor,\n",
    "#         'params': {\n",
    "#             'windowSize': [25, 50, 100, 150, 200, 250], #[25, 250], #\n",
    "#             'neighbors': [10, 20, 50, 100], #[50, 100], #\n",
    "#             'gpu': [True]\n",
    "#         }\n",
    "#     },\n",
    "#     'IF': {\n",
    "#         'class': IsolationForest,\n",
    "#         'params': {\n",
    "#             'windowSize': [20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 250, 300, 350, 400, 450, 500, 550, 600]\n",
    "#         }\n",
    "#     },\n",
    "#     'KMeans': {\n",
    "#         'class': KMeans,\n",
    "#         'params': {\n",
    "#             'windowSize': [50, 100, 200, 500],\n",
    "#             'n_clusters': [10, 20, 50, 100, 200],\n",
    "#         }\n",
    "#     },\n",
    "#     'MP': {\n",
    "#         'class': MatrixProfile,\n",
    "#         'params': {\n",
    "#             'windowSize': [20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 250, 300, 350, 400, 450, 500, 550, 600]\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# def run_experiment(base_learner_name, base_learner_class, params, ucr_path, results_path, scores_path):\n",
    "#     \"\"\"Run experiment for a single base learner with given parameters.\"\"\"\n",
    "#     learner = base_learner_class(**params)\n",
    "#     benchmark_results_file = os.path.join(results_path, f\"{learner.toString()}.csv\")\n",
    "#     # benchmark_scores_dir = os.path.join(scores_path, f\"{learner.toString()}\")\n",
    "        \n",
    "#     # Load results\n",
    "#     summary = pd.read_csv(benchmark_results_file, nrows=1)\n",
    "    \n",
    "    \n",
    "#     return {\n",
    "#         'params': params,\n",
    "#         'ucr_score': summary['accuracy'].values[0],\n",
    "#         'computational_time': summary['total_time'].values[0],\n",
    "#     }\n",
    "\n",
    "# def generate_param_combinations(param_dict):\n",
    "#     \"\"\"Generate all combinations of parameters.\"\"\"\n",
    "#     keys = list(param_dict.keys())\n",
    "#     values = list(param_dict.values())\n",
    "#     for instance in itertools.product(*values):\n",
    "#         yield dict(zip(keys, instance))\n",
    "\n",
    "# # Main experiment loop\n",
    "# ucr_path = 'ucrdata'\n",
    "# results_path = 'results'\n",
    "# scores_path = 'scores'\n",
    "\n",
    "# all_results = []\n",
    "\n",
    "# for base_learner_name, base_learner_info in base_learners.items():\n",
    "#     print(f\"Running experiments for {base_learner_name}\")\n",
    "#     base_learner_class = base_learner_info['class']\n",
    "#     param_combinations = list(generate_param_combinations(base_learner_info['params']))\n",
    "    \n",
    "#     for params in tqdm(param_combinations, desc=f\"{base_learner_name} configurations\"):\n",
    "#         result = run_experiment(base_learner_name, base_learner_class, params, ucr_path, results_path, scores_path)\n",
    "#         result['base_learner'] = base_learner_name\n",
    "#         all_results.append(result)\n",
    "\n",
    "# # Convert results to DataFrame\n",
    "# results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# # Analysis\n",
    "# for base_learner_name in base_learners.keys():\n",
    "#     learner_results = results_df[results_df['base_learner'] == base_learner_name]\n",
    "    \n",
    "#     print(f\"\\nAnalysis for {base_learner_name}:\")\n",
    "#     for metric in ['ucr_score', 'computational_time']:\n",
    "#         mean = learner_results[metric].mean()\n",
    "#         std = learner_results[metric].std()\n",
    "#         cv = std / mean\n",
    "#         print(f\"{metric}:\")\n",
    "#         print(f\"  Mean: {mean:.4f}\")\n",
    "#         print(f\"  Std Dev: {std:.4f}\")\n",
    "#         print(f\"  Coeff of Variation: {cv:.4f}\")\n",
    "    \n",
    "#     best_config = learner_results.loc[learner_results['ucr_score'].idxmax()]\n",
    "#     print(f\"\\nBest configuration for {base_learner_name}:\")\n",
    "#     print(best_config['params'])\n",
    "#     print(f\"UCR Score: {best_config['ucr_score']:.4f}\")\n",
    "\n",
    "# # Save full results\n",
    "# experiments_path = os.path.join('experiments', 'experiment_e1_results.csv')\n",
    "# results_df.to_csv(experiments_path, index=False)\n",
    "# print(f\"\\nFull results saved to {experiments_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained a diverse set of algorithms.  Diversity was introduced in two ways, namely:\n",
    "1. Multiple hyper-parameter congiurations (homogeneous and heterogneous ensembles)\n",
    "2. Multiple model classes (heterogeneous ensembles only)\n",
    "\n",
    "A very practical consideration for real-world TSAD is that the performance of a given model cannot typically be validated.  To simluate this lack of prior knowledge on the performance of a model, we randomly select 20 (or 30 since this is what the SoTA used) models for each ensemble learner and determine performance.   We repeat this process for a total of 30 repitions, in order to draw general conclusions from the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Load the data\n",
    "# df = pd.read_csv('experiments/experiment_e1_results.csv')\n",
    "\n",
    "# # Convert params column from string to dictionary\n",
    "# df['params'] = df['params'].apply(eval)\n",
    "\n",
    "# # 1. Line Plots\n",
    "# plt.figure(figsize=(20, 15))\n",
    "\n",
    "# for i, learner in enumerate(['LOF', 'IF', 'KMeans', 'MP'], 1):\n",
    "#     plt.subplot(2, 2, i)\n",
    "#     learner_df = df[df['base_learner'] == learner]\n",
    "    \n",
    "#     if learner == 'LOF':\n",
    "#         for neighbors in learner_df['params'].apply(lambda x: x['neighbors']).unique():\n",
    "#             data = learner_df[learner_df['params'].apply(lambda x: x['neighbors'] == neighbors)]\n",
    "#             plt.plot(data['params'].apply(lambda x: x['windowSize']), data['ucr_score'], \n",
    "#                      label=f'neighbors={neighbors}')\n",
    "#     elif learner == 'KMeans':\n",
    "#         for n_clusters in learner_df['params'].apply(lambda x: x['n_clusters']).unique():\n",
    "#             data = learner_df[learner_df['params'].apply(lambda x: x['n_clusters'] == n_clusters)]\n",
    "#             plt.plot(data['params'].apply(lambda x: x['windowSize']), data['ucr_score'], \n",
    "#                      label=f'n_clusters={n_clusters}')\n",
    "#     else:  # This covers both IF and MP\n",
    "#         plt.plot(learner_df['params'].apply(lambda x: x['windowSize']), learner_df['ucr_score'], \n",
    "#                  label=learner)\n",
    "    \n",
    "#     plt.title(f'{learner} Performance')\n",
    "#     plt.xlabel('Window Size')\n",
    "#     plt.ylabel('UCR Score')\n",
    "#     plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('visualisations/line_plots.png')\n",
    "# plt.close()\n",
    "\n",
    "# # 2. Scatter Plot\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# for learner in df['base_learner'].unique():\n",
    "#     learner_df = df[df['base_learner'] == learner]\n",
    "#     plt.scatter(learner_df['computational_time'], learner_df['ucr_score'], label=learner, alpha=0.7)\n",
    "\n",
    "# plt.xlabel('Computational Time')\n",
    "# plt.ylabel('UCR Score')\n",
    "# plt.title('Performance vs Computational Time')\n",
    "# plt.legend()\n",
    "# plt.savefig('visualisations/scatter_plot.png')\n",
    "# plt.close()\n",
    "\n",
    "# # 3. Box Plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.boxplot(x='base_learner', y='ucr_score', data=df)\n",
    "# plt.title('Distribution of UCR Scores by Base Learner')\n",
    "# plt.savefig('visualisations/box_plot.png')\n",
    "# plt.close()\n",
    "\n",
    "# # 4. Summary Table\n",
    "# summary = df.groupby('base_learner').agg({\n",
    "#     'ucr_score': ['max', 'mean', 'std'],\n",
    "#     'computational_time': 'mean'\n",
    "# }).reset_index()\n",
    "\n",
    "# summary.columns = ['Base Learner', 'Best UCR Score', 'Mean UCR Score', 'Std UCR Score', 'Mean Computational Time']\n",
    "\n",
    "# # Get parameters for best score\n",
    "# best_params = df.loc[df.groupby('base_learner')['ucr_score'].idxmax(), ['base_learner', 'params']]\n",
    "# best_params.columns = ['Base Learner', 'Best Parameters']\n",
    "\n",
    "# # Merge summary and best_params\n",
    "# summary = summary.merge(best_params, on='Base Learner')\n",
    "\n",
    "# # Calculate best performance-time trade-off (you may want to adjust this metric)\n",
    "# df['trade_off'] = df['ucr_score'] / df['computational_time']\n",
    "# best_trade_off = df.loc[df.groupby('base_learner')['trade_off'].idxmax(), ['base_learner', 'params']]\n",
    "# best_trade_off.columns = ['Base Learner', 'Best Trade-off Parameters']\n",
    "\n",
    "# # Merge summary and best_trade_off\n",
    "# summary = summary.merge(best_trade_off, on='Base Learner')\n",
    "\n",
    "# # Save summary to CSV\n",
    "# summary.to_csv('visualisations/summary_table.csv', index=False)\n",
    "\n",
    "# print(\"Visualizations and summary table have been created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_table = pd.read_csv('visualisations/summary_table.csv')\n",
    "# summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataloader import DataLoader, TimeSeries\n",
    "from lof import LocalOutlierFactor\n",
    "from matrix_profile import MatrixProfile\n",
    "from kmeans import KMeans\n",
    "from isolation_forest import IsolationForest\n",
    "from ensemble_detector import EnsembleDetector\n",
    "from benchmarker import benchmark\n",
    "\n",
    "# Define the path to your UCR dataset\n",
    "UCR_PATH = 'ucrdata'\n",
    "cached_scores_dir = 'scores'\n",
    "\n",
    "# Initialize DataLoader\n",
    "dataloader = DataLoader()\n",
    "\n",
    "# Create base learners\n",
    "lof1 = LocalOutlierFactor(windowSize=50, neighbors=50, gpu=True)\n",
    "lof2 = LocalOutlierFactor(windowSize=50, neighbors=100, gpu=True)\n",
    "mp = MatrixProfile(windowSize=140)\n",
    "km = KMeans(windowSize=50, n_clusters=100)\n",
    "if_detector = IsolationForest(windowSize=100)\n",
    "\n",
    "# Example 1: Homogeneous Ensemble\n",
    "homogeneous_ensemble = EnsembleDetector(\n",
    "    base_learners=[lof1, lof2],\n",
    "    method='simple_average',\n",
    "    scores_dir = cached_scores_dir\n",
    ")\n",
    "\n",
    "# Example 2: Heterogeneous Ensemble\n",
    "heterogeneous_ensemble = EnsembleDetector(\n",
    "    base_learners=[lof1, mp, km, if_detector],\n",
    "    method='wv_ols_r2',\n",
    "    method_params={'emphasize_diversity': False},\n",
    "    scores_dir = cached_scores_dir\n",
    ")\n",
    "\n",
    "# Example 3: Ensemble with top-k method\n",
    "topk_ensemble = EnsembleDetector(\n",
    "    base_learners=[lof1, lof2, mp, km, if_detector],\n",
    "    method='wv_ols_r2_topk',\n",
    "    method_params={'k': 3},\n",
    "    scores_dir = cached_scores_dir\n",
    ")\n",
    "\n",
    "# Load a time series from the UCR dataset\n",
    "ts_file = '001_UCR_Anomaly_DISTORTED1sddb40.csv'\n",
    "ts = dataloader.load_file(f\"{UCR_PATH}/{ts_file}\")\n",
    "\n",
    "# Fit and predict using the ensembles\n",
    "homogeneous_ensemble.fit(ts)\n",
    "het_ts, het_time = heterogeneous_ensemble.predict(ts)\n",
    "\n",
    "print(f\"Heterogeneous Ensemble Name: {heterogeneous_ensemble.toString()}\")\n",
    "print(f\"Prediction Time: {het_time:.2f} seconds\")\n",
    "\n",
    "# Run benchmark on the ensemble detector\n",
    "save_results_file = f\"ensembles/results/{heterogeneous_ensemble.toString()}.csv\"\n",
    "save_scores_dir = f\"ensembles/scores/{heterogeneous_ensemble.toString()}\"\n",
    "\n",
    "benchmark(heterogeneous_ensemble, UCR_PATH, save_results_file, save_scores_dir)\n",
    "\n",
    "# Analyze results\n",
    "\n",
    "\n",
    "results = pd.read_csv(save_results_file, nrows=1)\n",
    "accuracy = results['accuracy'].values[0]\n",
    "total_time = results['total_time'].values[0]\n",
    "\n",
    "print(f\"Ensemble Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Total Benchmark Time: {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleDetector(\n",
    "                    base_learners=base_learners,\n",
    "                    method=method,\n",
    "                    method_params=method_params,\n",
    "                    scores_dir=CACHED_SCORES_DIR\n",
    "                )\n",
    "                \n",
    "ensemble_name = ensemble.toString()\n",
    "save_results_file = os.path.join(ENSEMBLE_RESULTS_DIR, f\"{ensemble_name}_rep{rep}.csv\")\n",
    "save_scores_dir = os.path.join(ENSEMBLE_SCORES_DIR, f\"{ensemble_name}_rep{rep}\")\n",
    "                \n",
    "benchmark(ensemble, UCR_PATH, save_results_file, save_scores_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
