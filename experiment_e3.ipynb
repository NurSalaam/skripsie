{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# import random\n",
    "# import os\n",
    "# import json\n",
    "\n",
    "# from lof import LocalOutlierFactor\n",
    "# from matrix_profile import MatrixProfile\n",
    "# from isolation_forest import IsolationForest\n",
    "# from kmeans import KMeans\n",
    "# from ensemble_detector import EnsembleDetector\n",
    "# from dataloader import DataLoader\n",
    "# from benchmarker import benchmark\n",
    "\n",
    "# # Constants\n",
    "# UCR_PATH = 'ucrdata'\n",
    "# CACHED_SCORES_DIR = 'scores'\n",
    "# RESULTS_DIR = 'results'\n",
    "# ENSEMBLE_RESULTS_DIR = 'ensembles/results'\n",
    "# ENSEMBLE_SCORES_DIR = 'ensembles/scores'\n",
    "# N_REPETITIONS = 30\n",
    "# ENSEMBLE_SIZE = 20\n",
    "# BASE_LEARNERS_PER_TYPE = 5\n",
    "\n",
    "# # Base learner configurations\n",
    "# BASE_LEARNERS = {\n",
    "#     'LOF': {\n",
    "#         'class': LocalOutlierFactor,\n",
    "#         'params': [\n",
    "#             {'windowSize': ws, 'neighbors': n, 'gpu': True}\n",
    "#             for ws in [25, 50, 100, 150, 200, 250]\n",
    "#             for n in [10, 20, 50, 100]\n",
    "#         ]\n",
    "#     },\n",
    "#     'IF': {\n",
    "#         'class': IsolationForest,\n",
    "#         'params': [\n",
    "#             {'windowSize': ws}\n",
    "#             for ws in [20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 250, 300, 350, 400, 450, 500, 550, 600]\n",
    "#         ]\n",
    "#     },\n",
    "#     'KMeans': {\n",
    "#         'class': KMeans,\n",
    "#         'params': [\n",
    "#             {'windowSize': ws, 'n_clusters': nc}\n",
    "#             for ws in [50, 100, 200, 500]\n",
    "#             for nc in [10, 20, 50, 100, 200]\n",
    "#         ]\n",
    "#     },\n",
    "#     'MP': {\n",
    "#         'class': MatrixProfile,\n",
    "#         'params': [\n",
    "#             {'windowSize': ws}\n",
    "#             for ws in [20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 250, 300, 350, 400, 450, 500, 550, 600]\n",
    "#         ]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Ensemble methods\n",
    "# ENSEMBLE_METHODS = [\n",
    "#     ('simple_average', {}),\n",
    "#     ('maximum_score', {}),\n",
    "#     ('wv_ols', {}),\n",
    "#     ('wv_ols_r2', {'emphasize_diversity': False}),\n",
    "#     ('wv_ols_r2', {'emphasize_diversity': True}),\n",
    "#     ('hard_voting', {'spread_window': 100, 'gaussian': False}),\n",
    "#     ('hard_voting', {'spread_window': 100, 'gaussian': True})\n",
    "# ]\n",
    "\n",
    "# def create_heterogeneous_ensemble(n=ENSEMBLE_SIZE):\n",
    "#     base_learners = []\n",
    "#     selected_params = []\n",
    "    \n",
    "#     for base_learner_type in BASE_LEARNERS.keys():\n",
    "#         base_learner_class = BASE_LEARNERS[base_learner_type]['class']\n",
    "#         params_list = BASE_LEARNERS[base_learner_type]['params']\n",
    "#         type_params = random.sample(params_list, BASE_LEARNERS_PER_TYPE)\n",
    "        \n",
    "#         for params in type_params:\n",
    "#             base_learners.append(base_learner_class(**params))\n",
    "#             selected_params.append((base_learner_type, params))\n",
    "    \n",
    "#     return base_learners, selected_params\n",
    "\n",
    "# def run_experiment():\n",
    "#     results = []\n",
    "#     configurations = []\n",
    "    \n",
    "#     for rep in tqdm(range(N_REPETITIONS), desc=\"Running heterogeneous ensembles\"):\n",
    "#         base_learners, selected_params = create_heterogeneous_ensemble()\n",
    "        \n",
    "#         for method, method_params in ENSEMBLE_METHODS:\n",
    "#             ensemble = EnsembleDetector(\n",
    "#                 base_learners=base_learners,\n",
    "#                 method=method,\n",
    "#                 method_params=method_params,\n",
    "#                 scores_dir=CACHED_SCORES_DIR\n",
    "#             )\n",
    "            \n",
    "#             ensemble_name = ensemble.toString()\n",
    "#             save_results_file = os.path.join(ENSEMBLE_RESULTS_DIR, f\"{ensemble_name}_rep{rep}.csv\")\n",
    "#             save_scores_dir = os.path.join(ENSEMBLE_SCORES_DIR, f\"{ensemble_name}_rep{rep}\")\n",
    "            \n",
    "#             benchmark(ensemble, UCR_PATH, save_results_file, save_scores_dir)\n",
    "            \n",
    "#             benchmark_results = pd.read_csv(save_results_file, nrows=1)\n",
    "            \n",
    "#             result = {\n",
    "#                 'ensemble_method': method,\n",
    "#                 'method_params': json.dumps(method_params),  # Convert dict to string for storage in DataFrame\n",
    "#                 'repetition': rep,\n",
    "#                 'ucr_score': benchmark_results['accuracy'].values[0],\n",
    "#                 'computational_time': benchmark_results['total_time'].values[0]\n",
    "#             }\n",
    "#             results.append(result)\n",
    "            \n",
    "#             configuration = {\n",
    "#                 'ensemble_method': method,\n",
    "#                 'method_params': method_params,\n",
    "#                 'repetition': rep,\n",
    "#                 'base_learner_params': selected_params\n",
    "#             }\n",
    "#             configurations.append(configuration)\n",
    "    \n",
    "#     return pd.DataFrame(results), configurations\n",
    "\n",
    "# def analyze_results(results_df):\n",
    "#     summary = results_df.groupby(['ensemble_method', 'method_params']).agg({\n",
    "#         'ucr_score': ['mean', 'std', 'max'],\n",
    "#         'computational_time': ['mean', 'std']\n",
    "#     }).reset_index()\n",
    "    \n",
    "#     summary.columns = ['ensemble_method', 'method_params', 'mean_score', 'std_score', 'max_score', 'mean_time', 'std_time']\n",
    "#     summary['cv_score'] = summary['std_score'] / summary['mean_score']\n",
    "    \n",
    "#     return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running heterogeneous ensembles:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Running on CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking: 100%|██████████| 250/250 [06:19<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark complete for ensemble_het_if_km_lof_mp_simple_average:\n",
      "Total time: 5449.22 seconds\n",
      "Accuracy: 55.20%\n",
      "Correct predictions: 138\n",
      "Incorrect predictions: 112\n",
      "Failed predictions: 0\n",
      "Results saved to: ensembles/results/ensemble_het_if_km_lof_mp_simple_average_rep0.csv\n",
      "Scores saved to: ensembles/scores/ensemble_het_if_km_lof_mp_simple_average_rep0\n",
      "CUDA is not available. Running on CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Run the experiment\n",
    "if not os.path.exists(ENSEMBLE_RESULTS_DIR):\n",
    "    os.makedirs(ENSEMBLE_RESULTS_DIR)\n",
    "if not os.path.exists(ENSEMBLE_SCORES_DIR):\n",
    "    os.makedirs(ENSEMBLE_SCORES_DIR)\n",
    "\n",
    "results_df, configurations = run_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_csv('experiments/experiment_e3_results.csv', index=False)\n",
    "\n",
    "# # Save configurations\n",
    "# with open('experiments/experiment_e3_configurations.json', 'w') as f:\n",
    "#     json.dump(configurations, f, indent=2)\n",
    "\n",
    "# # Analyze the results\n",
    "# summary = analyze_results(results_df)\n",
    "# summary.to_csv('experiments/experiment_e3_summary.csv', index=False)\n",
    "\n",
    "# print(\"Experiment E3 completed. Results saved to 'experiments/experiment_e3_results.csv', 'experiments/experiment_e3_configurations.json', and 'experiments/experiment_e3_summary.csv'.\")\n",
    "\n",
    "# # Load and print summary\n",
    "# summary = pd.read_csv('experiments/experiment_e3_summary.csv')\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All visualizations for Experiment 3 have been created and saved in the 'visualisations/experiment_3' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Helper function to create unique method names\n",
    "def get_unique_method_name(method, params):\n",
    "    if method == 'wv_ols_r2':\n",
    "        return f\"wv_ols_r2_{str(params.get('emphasize_diversity', False))[0]}\"\n",
    "    elif method == 'hard_voting':\n",
    "        return f\"hard_voting_{str(params.get('gaussian', False))[0]}\"\n",
    "    elif params:\n",
    "        param_str = '_'.join(f\"{k}_{v}\" for k, v in sorted(params.items()))\n",
    "        return f\"{method}_{param_str}\"\n",
    "    return method\n",
    "\n",
    "# Load the data\n",
    "e3_results = pd.read_csv('experiments/experiment_e3_results.csv')\n",
    "e3_summary = pd.read_csv('experiments/experiment_e3_summary.csv')\n",
    "\n",
    "# Load configurations\n",
    "with open('experiments/experiment_e3_configurations.json', 'r') as f:\n",
    "    e3_configs = json.load(f)\n",
    "\n",
    "# Create unique method names\n",
    "e3_results['unique_method'] = e3_results.apply(lambda row: get_unique_method_name(row['ensemble_method'], json.loads(row['method_params'])), axis=1)\n",
    "e3_summary['unique_method'] = e3_summary.apply(lambda row: get_unique_method_name(row['ensemble_method'], json.loads(row['method_params'])), axis=1)\n",
    "\n",
    "# Calculate relative performance for each ensemble\n",
    "relative_performances = []\n",
    "for config in e3_configs:\n",
    "    method = config['ensemble_method']\n",
    "    rep = config['repetition']\n",
    "    constituent_scores = [e1_results[(e1_results['base_learner'] == bl_type) & \n",
    "                                     (e1_results['params'] == str(params))]['ucr_score'].values[0]\n",
    "                          for bl_type, params in config['base_learner_params']]\n",
    "    avg_constituent_score = np.mean(constituent_scores)\n",
    "    unique_method = get_unique_method_name(method, config['method_params'])\n",
    "    ensemble_data = e3_results[(e3_results['ensemble_method'] == method) & \n",
    "                               (e3_results['repetition'] == rep)]\n",
    "    ensemble_score = ensemble_data['ucr_score'].values[0]\n",
    "    relative_score = ensemble_score / avg_constituent_score\n",
    "    relative_performances.append({\n",
    "        'unique_method': unique_method,\n",
    "        'repetition': rep,\n",
    "        'relative_score': relative_score,\n",
    "        'ensemble_score': ensemble_score,\n",
    "        'avg_constituent_score': avg_constituent_score\n",
    "    })\n",
    "\n",
    "relative_df = pd.DataFrame(relative_performances)\n",
    "\n",
    "# 1. Box Plot of UCR Scores\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(x='unique_method', y='ucr_score', data=e3_results)\n",
    "plt.title('Distribution of UCR Scores by Ensemble Method')\n",
    "plt.xlabel('Ensemble Method')\n",
    "plt.ylabel('UCR Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualisations/experiment_3/ucr_scores_boxplot.png')\n",
    "plt.close()\n",
    "\n",
    "# 2. Performance Improvement Violin Plot\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.violinplot(x='unique_method', y='relative_score', data=relative_df)\n",
    "plt.axhline(y=1, color='r', linestyle='--')\n",
    "plt.title('Ensemble Performance Relative to Constituent Learners')\n",
    "plt.xlabel('Ensemble Method')\n",
    "plt.ylabel('Relative Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualisations/experiment_3/relative_performance_violin.png')\n",
    "plt.close()\n",
    "\n",
    "# 3. Performance Improvement Bar Plot\n",
    "plt.figure(figsize=(20, 12))\n",
    "mean_relative_scores = relative_df.groupby('unique_method')['relative_score'].mean()\n",
    "std_relative_scores = relative_df.groupby('unique_method')['relative_score'].std()\n",
    "\n",
    "mean_relative_scores.plot(kind='bar', yerr=std_relative_scores, capsize=5)\n",
    "plt.axhline(y=1, color='r', linestyle='--')\n",
    "plt.title('Average Ensemble Performance Relative to Constituent Learners')\n",
    "plt.xlabel('Ensemble Method')\n",
    "plt.ylabel('Average Relative Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualisations/experiment_3/relative_performance_bar.png')\n",
    "plt.close()\n",
    "\n",
    "# 2. Performance Improvement Heatmap\n",
    "avg_scores = e3_summary.groupby('unique_method')['mean_score'].mean()\n",
    "improvement_pivot = pd.DataFrame({'Improvement': (avg_scores - avg_scores.mean()) / avg_scores.mean() * 100})\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['#FF0000', '#FFFFFF', '#00FF00']  # Red, White, Green\n",
    "cmap = LinearSegmentedColormap.from_list('custom', colors, N=100)\n",
    "\n",
    "sns.heatmap(improvement_pivot, annot=True, fmt='.2f', cmap=cmap, center=0)\n",
    "plt.title('Performance Improvement of Ensemble Methods (%)')\n",
    "plt.xlabel('Ensemble Method')\n",
    "plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualisations/experiment_3/performance_improvement_heatmap.png')\n",
    "plt.close()\n",
    "\n",
    "# 3. Reliability Bar Chart\n",
    "plt.figure(figsize=(15, 10))\n",
    "cv_scores = e3_summary.set_index('unique_method')['cv_score']\n",
    "cv_scores.plot(kind='bar')\n",
    "plt.title('Coefficient of Variation of UCR Scores by Ensemble Method')\n",
    "plt.xlabel('Ensemble Method')\n",
    "plt.ylabel('Coefficient of Variation')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualisations/experiment_3/reliability_bar_chart.png')\n",
    "plt.close()\n",
    "\n",
    "# 4. Computational Time vs. Performance Scatter Plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.scatterplot(x='computational_time', y='ucr_score', hue='unique_method', data=e3_results)\n",
    "plt.xlabel('Computational Time')\n",
    "plt.ylabel('UCR Score')\n",
    "plt.title('Performance vs Computational Time: Heterogeneous Ensemble Methods')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualisations/experiment_3/performance_vs_time_scatter.png')\n",
    "plt.close()\n",
    "\n",
    "# 5. Performance Distribution Violin Plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.violinplot(x='unique_method', y='ucr_score', data=e3_results)\n",
    "plt.title('Distribution of UCR Scores by Ensemble Method')\n",
    "plt.xlabel('Ensemble Method')\n",
    "plt.ylabel('UCR Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualisations/experiment_3/performance_distribution_violin.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"All visualizations for Experiment 3 have been created and saved in the 'visualisations/experiment_3' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
